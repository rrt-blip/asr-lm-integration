@misc{ACOUSTICMODELINGPROBLEMAUTOMATIC,
  title = {{{THE ACOUSTIC-MODELING PROBLEM IN AUTOMATIC SPEECH RECOGNITION}} - {{ProQuest}}},
  urldate = {2025-10-08},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  howpublished = {https://www.proquest.com/openview/e6f780423dbb9f22b0b3bba6f0bd9990/1?cbl=18750\&diss=y\&pq-origsite=gscholar},
  langid = {english},
  file = {/Users/rritahajrizi/Zotero/storage/26XIR4IV/1.html}
}

@misc{CC100MonolingualDatasets,
  title = {{{CC-100}}: {{Monolingual Datasets}} from {{Web Crawl Data}}},
  urldate = {2025-10-23},
  howpublished = {https://data.statmt.org/cc-100/},
  keywords = {reference},
  file = {/Users/rritahajrizi/Zotero/storage/YAPHAIZ4/cc-100.html}
}

@misc{gravesGeneratingSequencesRecurrent2014,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = 2014,
  month = jun,
  number = {arXiv:1308.0850},
  eprint = {1308.0850},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1308.0850},
  urldate = {2025-10-14},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/rritahajrizi/Zotero/storage/RNIIVSW4/Graves - 2014 - Generating Sequences With Recurrent Neural Networks.pdf;/Users/rritahajrizi/Zotero/storage/EMMV7QGU/1308.html}
}

@misc{Lauragpt_citation,
  title = {{{LauraGPT}}: {{Listen}}, {{Attend}}, {{Understand}}, and {{Regenerate Audio}} with {{GPT}}},
  shorttitle = {{{LauraGPT}}},
  author = {Du, Zhihao and Wang, Jiaming and Chen, Qian and Chu, Yunfei and Gao, Zhifu and Li, Zerui and Hu, Kai and Zhou, Xiaohuan and Xu, Jin and Ma, Ziyang and Wang, Wen and Zheng, Siqi and Zhou, Chang and Yan, Zhijie and Zhang, Shiliang},
  year = 2024,
  month = jul,
  number = {arXiv:2310.04673},
  eprint = {2310.04673},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.04673},
  urldate = {2025-09-24},
  abstract = {Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/rritahajrizi/Zotero/storage/BBKJRZV9/Du et al. - 2024 - LauraGPT Listen, Attend, Understand, and Regenerate Audio with GPT.pdf;/Users/rritahajrizi/Zotero/storage/AGD486L4/2310.html}
}

@inproceedings{leDeepShallowFusion2021,
  title = {Deep {{Shallow Fusion}} for {{RNN-T Personalization}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Le, Duc and Keren, Gil and Chan, Julian and Mahadeokar, Jay and Fuegen, Christian and Seltzer, Michael L.},
  year = 2021,
  month = jan,
  pages = {251--257},
  doi = {10.1109/SLT48900.2021.9383560},
  urldate = {2025-10-23},
  abstract = {End-to-end models in general, and Recurrent Neural Network Transducer (RNN-T) in particular, have gained significant traction in the automatic speech recognition community in the last few years due to their simplicity, compactness, and excellent performance on generic transcription tasks. However, these models are more challenging to personalize compared to traditional hybrid systems due to the lack of external language models and difficulties in recognizing rare long-tail words, specifically entity names. In this work, we present novel techniques to improve RNN-T's ability to model rare WordPieces, infuse extra information into the encoder, enable the use of alternative graphemic pronunciations, and perform deep fusion with personalized language models for more robust biasing. We show that these combined techniques result in 15.4\%-34.5\% relative Word Error Rate improvement compared to a strong RNN-T baseline which uses shallow fusion and text-to-speech augmentation. Our work helps push the boundary of RNN-T personalization and close the gap with hybrid systems on use cases where biasing and entity recognition are crucial.},
  keywords = {Automatic speech recognition,Conferences,contextual biasing,Error analysis,name recognition,Recurrent neural networks,RNN-T personalization,shallow fusion,Task analysis,Transducers},
  file = {/Users/rritahajrizi/Zotero/storage/TKTRWWSG/Le et al. - 2021 - Deep Shallow Fusion for RNN-T Personalization.pdf}
}

@incollection{mohriSpeechRecognitionWeighted2008,
  title = {Speech {{Recognition}} with {{Weighted Finite-State Transducers}}},
  booktitle = {Springer {{Handbook}} of {{Speech Processing}}},
  author = {Mohri, Mehryar and Pereira, Fernando and Riley, Michael},
  editor = {Benesty, Jacob and Sondhi, M. Mohan and Huang, Yiteng Arden},
  year = 2008,
  pages = {559--584},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-49127-9_28},
  urldate = {2025-09-27},
  abstract = {This chapter describes a general representation and algorithmic framework for speech recognition based on weighted finite-state transducers. These transducers provide a common and natural representation for major components of speech recognition systems, including hidden Markov models (HMMs), context-dependency models, pronunciation dictionaries, statistical grammars, and word or phone lattices. General algorithms for building and optimizing transducer models are presented, including composition for combining models, weighted determinization and minimization for optimizing time and space requirements, and a weight pushing algorithm for redistributing transition weights optimally for speech recognition. The application of these methods to large-vocabulary recognition tasks is explained in detail, and experimental results are given, in particular for the North American Business News (NAB) task, in which these methods were used to combine HMMs, full cross-word triphones, a lexicon of forty thousand words, and a large trigram grammar into a single weighted transducer that is only somewhat larger than the trigram word grammar and that runs NAB in real-time on a very simple decoder. Another example demonstrates that the same methods can be used to optimize lattices for second-pass recognition.},
  isbn = {978-3-540-49125-5 978-3-540-49127-9},
  langid = {english},
  file = {/Users/rritahajrizi/Zotero/storage/XUB32WM6/Mohri et al. - 2008 - Speech Recognition with Weighted Finite-State Transducers.pdf}
}

@misc{pengSurveySpeechLarge2025,
  title = {A {{Survey}} on {{Speech Large Language Models}} for {{Understanding}}},
  author = {Peng, Jing and Wang, Yucheng and Li, Bohan and Guo, Yiwei and Wang, Hankun and Fang, Yangui and Xi, Yu and Li, Haoyu and Li, Xu and Zhang, Ke and Wang, Shuai and Yu, Kai},
  year = 2025,
  month = jul,
  number = {arXiv:2410.18908},
  eprint = {2410.18908},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.18908},
  urldate = {2025-09-23},
  abstract = {Speech understanding is essential for interpreting the diverse forms of information embedded in spoken language, including linguistic, paralinguistic, and non-linguistic cues that are vital for effective human-computer interaction. The rapid advancement of large language models (LLMs) has catalyzed the emergence of Speech Large Language Models (Speech LLMs), which marks a transformative shift toward general-purpose speech understanding systems. To further clarify and systematically delineate task objectives, in this paper, we formally define the concept of speech understanding and introduce a structured taxonomy encompassing its informational, functional, and format dimensions. Within this scope of definition, we present a comprehensive review of current Speech LLMs, analyzing their architectures through a three-stage abstraction: Modality Feature Extraction, Modality Information Fusion, and LLM Inference. In addition, we examine training strategies, discuss representative datasets, and review evaluation methodologies adopted in the field. Based on empirical analyses and experimental evidence, we identify two key challenges currently facing Speech LLMs: instruction sensitivity and degradation in semantic reasoning and propose concrete directions for addressing these issues. Through this systematic and detailed survey, we aim to offer a foundational reference for researchers and practitioners working toward more robust, generalizable, and human-aligned Speech LLMs.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/rritahajrizi/Zotero/storage/CNA5I8RW/Peng et al. - 2025 - A Survey on Speech Large Language Models for Understanding.pdf;/Users/rritahajrizi/Zotero/storage/D89JGQ6F/2410.html}
}

@misc{shuLLaSMLargeLanguage2023,
  title = {{{LLaSM}}: {{Large Language}} and {{Speech Model}}},
  shorttitle = {{{LLaSM}}},
  author = {Shu, Yu and Dong, Siwei and Chen, Guangyao and Huang, Wenhao and Zhang, Ruihua and Shi, Daochen and Xiang, Qiqi and Shi, Yemin},
  year = 2023,
  month = sep,
  number = {arXiv:2308.15930},
  eprint = {2308.15930},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.15930},
  urldate = {2025-09-23},
  abstract = {Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions dataset is available at https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/rritahajrizi/Zotero/storage/8PSVK4B3/Shu et al. - 2023 - LLaSM Large Language and Speech Model.pdf;/Users/rritahajrizi/Zotero/storage/GFVFMJRY/2308.html}
}

@inproceedings{toshniwalComparisonTechniquesLanguage2018,
  title = {A {{Comparison}} of {{Techniques}} for {{Language Model Integration}} in {{Encoder-Decoder Speech Recognition}}},
  booktitle = {2018 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Toshniwal, Shubham and Kannan, Anjuli and Chiu, Chung-Cheng and Wu, Yonghui and Sainath, Tara N and Livescu, Karen},
  year = 2018,
  month = dec,
  pages = {369--375},
  doi = {10.1109/SLT.2018.8639038},
  urldate = {2025-10-06},
  abstract = {Attention-based recurrent neural encoder-decoder models present an elegant solution to the automatic speech recognition problem. This approach folds the acoustic model, pronunciation model, and language model into a single network and requires only a parallel corpus of speech and text for training. However, unlike in conventional approaches that combine separate acoustic and language models, it is not clear how to use additional (unpaired) text. While there has been previous work on methods addressing this problem, a thorough comparison among methods is still lacking. In this paper, we compare a suite of past methods and some of our own proposed methods for using unpaired text data to improve encoder-decoder models. For evaluation, we use the medium-sized Switchboard data set and the large-scale Google voice search and dictation data sets. Our results confirm the benefits of using unpaired text across a range of methods and data sets. Surprisingly, for first-pass decoding, the rather simple approach of shallow fusion performs best across data sets. However, for Google data sets we find that cold fusion has a lower oracle error rate and outperforms other approaches after second-pass rescoring on the Google voice search data set.},
  keywords = {Acoustics,cold fusion,Computational modeling,Data models,Decoding,deep fusion,encoder-decoder,language model,Mathematical model,shallow fusion,speech recognition,Speech recognition,Training},
  file = {/Users/rritahajrizi/Zotero/storage/RWV49BKF/Toshniwal et al. - 2018 - A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition.pdf}
}

@misc{yangWhenLargeLanguage2025,
  title = {When {{Large Language Models Meet Speech}}: {{A Survey}} on {{Integration Approaches}}},
  shorttitle = {When {{Large Language Models Meet Speech}}},
  author = {Yang, Zhengdong and Shimizu, Shuichiro and Yu, Yahan and Chu, Chenhui},
  year = 2025,
  month = sep,
  number = {arXiv:2502.19548},
  eprint = {2502.19548},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.19548},
  urldate = {2025-09-23},
  abstract = {Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/rritahajrizi/Zotero/storage/ZI84V9BL/Yang et al. - 2025 - When Large Language Models Meet Speech A Survey on Integration Approaches.pdf;/Users/rritahajrizi/Zotero/storage/2PYWK6YP/2502.html}
}

@book{yuAutomaticSpeechRecognition2015,
  title = {Automatic {{Speech Recognition}}: {{A Deep Learning Approach}}},
  shorttitle = {Automatic {{Speech Recognition}}},
  author = {Yu, Dong and Deng, Li},
  year = 2015,
  series = {Signals and {{Communication Technology}}},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-4471-5779-3},
  urldate = {2025-10-18},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-1-4471-5778-6 978-1-4471-5779-3},
  langid = {english},
  file = {/Users/rritahajrizi/Zotero/storage/7QKK4QB2/Yu and Deng - 2015 - Automatic Speech Recognition A Deep Learning Approach.pdf}
}

@misc{zhaoSurveyLargeLanguage2025,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = 2025,
  month = mar,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.18223},
  urldate = {2025-09-23},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/rritahajrizi/Zotero/storage/AWRCTKZZ/Zhao et al. - 2025 - A Survey of Large Language Models.pdf;/Users/rritahajrizi/Zotero/storage/R3XPHV5E/2303.html}
}
